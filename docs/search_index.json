[
["multipel-linjär-regression.html", "Kapitel 12 Multipel linjär regression 12.1 Regression och lm() 12.2 \\(F\\)-test och anova() 12.3 Variabler med flera än två kategorier.", " Kapitel 12 Multipel linjär regression I en multipel linjär regressionsanalys inkorporeras flera oberoende variabel. Beroende på syftet väljer vi olika typer av modeller: Deskriptivt syfte: Målet är att beskriva samband och strukturer för att öka förståelsen för data. Här kan vi inkludera variabler som är relevanta utifrån ett beskrivande syfte, till exempel om vi studerar hur kriminalitet varierar mellan socio-ekonomiska kategorier. Prediktivt syfte: Målet är att inkludera variabler som ger god prediktionsförmåga. Koefficienterna för enskilda variabler är ej av intresse. Vi är inte intresserade av att förstå en struktur. Kausalt syfte: Målet är att studera den kausala effekten av en behandlingsvariabel på ett utfall. Övriga variabler som inkluderas är så kallade kontrollvariabler. Det är endast behandlingsvariabelns koefficient som är av intresse. Allmänt så kan modellen vid multipel linjär regression skriva \\[y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots \\beta_k x_k + \\varepsilon_i.\\] 12.1 Regression och lm() Med minstakvadrat-metoden ska vi nu anpassas ett regressionsplan som beskriver sambandet mellan benstyrka och längd. rm(list=ls()) df &lt;- read.csv2(&quot;D:/conscriptiondata_sample_final.csv&quot;) m1 &lt;- lm(legstrength ~ height, data=df) rm(list=ls()) df &lt;- read.csv2(&quot;D:/conscriptiondata_sample_final.csv&quot;) m1 &lt;- lm(legstrength ~ height, data=df) Den linjära regressionsanalysen är sparad i m1 som ger en högst kortfattad resultatredovisning, i princip enbart koefficienterna redovisas. 12.2 \\(F\\)-test och anova() Vi ska nu testa göra \\(F\\)-test i R. Ett test av nestade modeller är \\[F_{obs} = \\dfrac{(SSE_R - SSE_C)/(k-g)}{SSE_C/(n-k)}\\] där \\(SSE_R\\) är residualkvadratsumman i den reducerade modellen och \\(SSE_C\\) är residualkvadratsumman i den fulla modellen. Denna teststatistiska är \\(F\\)-fördelad med \\(k-g\\) respektive \\(n-k\\) frihetsgrader där \\(k\\) och \\(g\\) är antalet skattade parametrar i den fulla respektive reducerade modellen. Notera att differensen mellan residualkvadratsummorna alltid är positiv efterom den fulla modellen per definition innehåller fler variabler och således minskar alltid residualspridningen. Huruvida denna sänkning är tillräckligt stor för att vara relevant är en annan fråga. Anta nu att vi vill jämföra modellen \\[legstrength = \\beta_0 + \\beta_1 height\\] med \\[legstrength = \\beta_0 + \\beta_1 height + \\beta_2 weight + \\beta_3 armstrength\\] Det innebär att vi testar \\[H_0: \\beta_2 = \\beta_3 = 0\\] vs \\[H_1:\\text{Minst en av}\\,\\,\\, \\beta_2, \\beta_3\\neq 0\\] I R är proceduren att vi skattade en reducerad modell m_r och en full modell m_c. Därefter används funktionen anova() som genomför en så kallad variansanalys (analysis of variance) för ett eller flera regressionsobjekt. rm(list=ls()) df &lt;- read.csv2(&quot;D:/conscriptiondata_sample_final.csv&quot;)[1:50,] m_r &lt;- lm(legstrength ~ height, data=df) m_c &lt;- lm(legstrength ~ height + weight + armstrength, data=df) Nested_F_test_1 &lt;- anova(m_r, m_c) Nested_F_test_1 &gt; Analysis of Variance Table &gt; &gt; Model 1: legstrength ~ height &gt; Model 2: legstrength ~ height + weight + armstrength &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 48 998734 &gt; 2 46 701529 2 297206 9.7441 0.0002963 *** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 I utskriften ges följande information: Antal frihetsgrader respektive residualkvadratsumman i den reducerade modellen (Model 1) 48 och 998734.3670512. Antal frihetsgrader respektive residualkvadratsumman i den fula modellen (Model 1) 46 och 701528.6153138. Differensen mellan modellernas frihetsgrader och residualkvadratsummor 2 och 297205.7517374. Slutligen presenteras \\(F_{obs}\\) 9.7440534 och \\(p\\)-värdet 0.0002963. Som illustration relaterar vi detta till den manuella beräkningen. I praktiken används dock anova-funktionen. # Residualkvadratsumma i reducerad modell SSER &lt;- sum(m_r$residuals^2) # Residualkvadratsumma i full modell SSEC &lt;- sum(m_c$residuals^2) # Skillnad i frihetsgrader mellan modellerna df1 &lt;- m_r$df - m_c$df # Antal frihetsgrader mellan modellerna df2 &lt;- m_c$df # Observerat F-värde Fobs &lt;- ((SSER - SSEC)/df1) / (SSEC/df2) Fobs &gt; [1] 9.744053 # P-värde från F-fördelningen under nollhypotesen pvalue &lt;- (1 - pf(Fobs, df1, df2)) pvalue &gt; [1] 0.0002962744 12.3 Variabler med flera än två kategorier. Vi har tidigare sett på binära variabler, men nu ska vi undersöka hur kategorivariabler med fler än 2 kategorier kan inkluderas i en analys. Koda om till dummy-variabler (0-1), där dummy-variabel indikerar en kategori. Om en variabel har 3 kategorier således 3 dummy-variabler. Dummy-variabeln som indikerar referenskategorin inkluderas INTE i regressionsmodellen. Använd psych_cat som är en faktor-variabel. 12.3.1 Dummy-variabler vi ska börja med att metoden med omkodning till dummy-variabler och väljer sedan den lägsta kategorin som referens. Det finns en numerisk variabel som heter psych3bestående av värdena 1,2,3 som vi nu kodar som vill binära dummy-variabler. df$psych_d1 &lt;- NA df$psych_d1[df$psych3 == 1] &lt;- 1 df$psych_d1[df$psych3 == 2] &lt;- 0 df$psych_d1[df$psych3 == 3] &lt;- 0 df$psych_d2 &lt;- NA df$psych_d2[df$psych3 == 1] &lt;- 0 df$psych_d2[df$psych3 == 2] &lt;- 1 df$psych_d2[df$psych3 == 3] &lt;- 0 df$psych_d3 &lt;- NA df$psych_d3[df$psych3 == 1] &lt;- 0 df$psych_d3[df$psych3 == 2] &lt;- 0 df$psych_d3[df$psych3 == 3] &lt;- 1 m_psychd &lt;- lm(legstrength ~ psych_d2 + psych_d3, data = df) sum_m_psychd &lt;- summary(m_psychd) sum_m_psychd &gt; &gt; Call: &gt; lm(formula = legstrength ~ psych_d2 + psych_d3, data = df) &gt; &gt; Residuals: &gt; Min 1Q Median 3Q Max &gt; -302.87 -60.02 -14.07 83.16 362.73 &gt; &gt; Coefficients: &gt; Estimate Std. Error t value Pr(&gt;|t|) &gt; (Intercept) 509.65 68.03 7.491 0.00000000148 *** &gt; psych_d2 66.62 71.06 0.938 0.35328 &gt; psych_d3 332.40 117.83 2.821 0.00699 ** &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; &gt; Residual standard error: 136.1 on 47 degrees of freedom &gt; Multiple R-squared: 0.1534, Adjusted R-squared: 0.1173 &gt; F-statistic: 4.257 on 2 and 47 DF, p-value: 0.01999 I ser att referenskategorin är de med lägst psykologisk bedömningsförmåga. Tolkningen är att jämfört med de med låg psykologisk bedömningsföråga så har de med medium psykologisk bedömningsförmåga i genomsnitt 66.6 Newton högre benstyrka. De med hög psykologisk bedömningsförmåga i genomsnitt 332.4 Newton högre benstyrka jämfört med låg psykologiskt bedömningsförmåga. Observera att alla jämförelser görs mot referenskategorin! Alla jämförelser görs mot referenskategorin, men hur avgöra om variabeln psykologiskt bedömning har betydelse överhuvudtaget? När vi använder dummy-variabler är det uppenbart att lm() hanterar kategorierna som olika variabler, till exempel skattar vi inte en parameter utan 2 om variabeln består av tre kategorier. Notera att utskrifterna är i princip identiska och modellen ger oss ett \\(F\\)-värde och ett p-värde 0.01999. Detta är ett resultatet från ett nestat \\(F\\)-test där den reducerade modellen enbart består av interceptet. Om vi har flera variabler i modellen och vi önskar att testa om kategorivariabel används därefter \\(F\\)-test analoigt med tidigare m2_r &lt;- lm(legstrength ~ height + weight + armstrength, data=df) m2_c &lt;- lm(legstrength ~ height + weight + armstrength + psych_d2 + psych_d3, data=df) Nested_F_test_2 &lt;- anova(m2_r, m2_c) Nested_F_test_2 &gt; Analysis of Variance Table &gt; &gt; Model 1: legstrength ~ height + weight + armstrength &gt; Model 2: legstrength ~ height + weight + armstrength + psych_d2 + psych_d3 &gt; Res.Df RSS Df Sum of Sq F Pr(&gt;F) &gt; 1 46 701529 &gt; 2 44 602077 2 99452 3.634 0.03462 * &gt; --- &gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 12.3.2 Faktor-variabler I data frame finns dock redan variabeln psych3_cat som är psykologisk bedömning kodat faktorvariabel. Jämför med psych3 och psych3_cat. m_psychcat &lt;- lm(legstrength ~ psych3_cat, data = df) m_psychcat &gt; &gt; Call: &gt; lm(formula = legstrength ~ psych3_cat, data = df) &gt; &gt; Coefficients: &gt; (Intercept) psych3_catLow (1-2) psych3_catMedium (3-7) &gt; 842.1 -332.4 -265.8 Vi ser att resultatet skiljer sig åt jämfört med analysen med dummy-variabel. Det beror på att referensvariabeln i en faktor-variabel i detta fall är bestämt till den högsta psykolisk bedömningen. Tolkningen är att jämfört med de med hög psykologisk bedömningsföråga så har de med låg psykologisk bedömningsförmåga i genomsnitt -332.4 Newton lägre benstyrka. Vi använder revel() för att ändra faktorns referens. df$psych3_cat &lt;- relevel(df$psych3_cat, ref = &quot;Low (1-2)&quot;) m2_psychcat &lt;- lm(legstrength ~ psych3_cat, data = df) m2_psychcat &gt; &gt; Call: &gt; lm(formula = legstrength ~ psych3_cat, data = df) &gt; &gt; Coefficients: &gt; (Intercept) psych3_catHigh (8-9 psych3_catMedium (3-7) &gt; 509.65 332.40 66.62 Visserligen ordningen på parameterskattningarna annorlunda i resultatet, men vi ser att värdena nu är samma som med dummy-varabler. "]
]
